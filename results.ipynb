{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reproduces the key figures and experiments from our [writeup on Induction Heads](https://www.notion.so/Evaluating-Anthropic-s-Induction-Head-Claims-a84793d55332409392e488d2e8b620bd). Very little exposition is done here, and the reader is advised to follow along with the writeup. All section titles other than <i>Setup</i> match the corresponding sections of the writeup."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import run_experiment\n",
    "from experiments import make_experiments, make_make_corr, FixedSampler\n",
    "from utils import compare_saa_in_cui\n",
    "from interp.circuit.causal_scrubbing.hypothesis import (\n",
    "    CondSampler,\n",
    "    ExactSampler,\n",
    "    UncondSampler,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entry point for running causal scrubbing experiments is the `experiments.py` file, which takes in command line arguments, builds experiment specifications, and passes them along to `main.py:run_experiments`. Specifications consist of a correspondence graph and, on many occasions, options for altering the model in various ways to enable the desired experiment.\n",
    "\n",
    "The following code block bypasses `experiments.py`'s command-line interface by duplicating some of its functionality and calling `main.py:run_experiments` directly in order to reproduce the values from the <i>Preliminary Experiments</i> section of the writeup.\n",
    "\n",
    "Note: we regrettably changed naming conventions a few times over the course of writing the many experiments specified in `experiments.py`. Those reproduced here have been renamed to match the descriptions in the writeup, but beyond those, the reader is advised not to assume the experiment's name is sufficient for understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Running experiment unscrubbed\n",
      "OVERALL\n",
      "     4.192    10.507   3000000\n",
      "CANDIDATES\n",
      "     3.263    11.428    163147\n",
      "LATER CANDIDATES\n",
      "     1.165     7.686     36312\n",
      "REPEATS\n",
      "     2.499     5.658   1278308\n",
      "UNCOMMON REPEATS\n",
      "     3.952    11.538    303884\n",
      "NON-ERB UNCOMMON REPEATS\n",
      "     5.569     9.985    172800\n",
      "MISLEADING INDUCTION\n",
      "     4.837     9.919    182439\n",
      "CANDIDATE ERB\n",
      "     0.202     0.538     30722\n",
      "NFERB UR\n",
      "     5.760     9.610    158631\n",
      "\n",
      "\n",
      "Running experiment ev\n",
      "OVERALL\n",
      "     4.270    10.392   3000000\n",
      "CANDIDATES\n",
      "     3.382    10.846    163147\n",
      "LATER CANDIDATES\n",
      "     1.558     6.768     36312\n",
      "REPEATS\n",
      "     2.675     6.037   1278308\n",
      "UNCOMMON REPEATS\n",
      "     4.556    11.424    303884\n",
      "NON-ERB UNCOMMON REPEATS\n",
      "     6.013     9.766    172800\n",
      "MISLEADING INDUCTION\n",
      "     4.788     9.673    182439\n",
      "CANDIDATE ERB\n",
      "     0.760     1.752     30722\n",
      "NFERB UR\n",
      "     6.172     9.489    158631\n",
      "\n",
      "\n",
      "Running experiment pth-k\n",
      "OVERALL\n",
      "     4.318    10.685   3000000\n",
      "CANDIDATES\n",
      "     3.462    10.952    163161\n",
      "LATER CANDIDATES\n",
      "     1.827     7.838     36311\n",
      "REPEATS\n",
      "     2.767     6.752   1278288\n",
      "UNCOMMON REPEATS\n",
      "     5.013    12.238    303868\n",
      "NON-ERB UNCOMMON REPEATS\n",
      "     6.339    10.281    172779\n",
      "MISLEADING INDUCTION\n",
      "     4.764     9.678    182416\n",
      "CANDIDATE ERB\n",
      "     1.099     3.698     30721\n",
      "NFERB UR\n",
      "     6.490     9.948    158612\n",
      "\n",
      "\n",
      "Running experiment eq\n",
      "OVERALL\n",
      "     4.346    10.338   3000000\n",
      "CANDIDATES\n",
      "     3.837    10.490    163129\n",
      "LATER CANDIDATES\n",
      "     3.339     9.473     36300\n",
      "REPEATS\n",
      "     2.855     6.438   1278282\n",
      "UNCOMMON REPEATS\n",
      "     5.230    10.152    303828\n",
      "NON-ERB UNCOMMON REPEATS\n",
      "     6.039     9.249    172782\n",
      "MISLEADING INDUCTION\n",
      "     4.748     9.564    182420\n",
      "CANDIDATE ERB\n",
      "     2.886     7.798     30711\n",
      "NFERB UR\n",
      "     6.112     9.201    158614\n",
      "\n",
      "\n",
      "Running experiment all\n",
      "OVERALL\n",
      "     4.448    10.882   3000000\n",
      "CANDIDATES\n",
      "     3.992    10.694    163161\n",
      "LATER CANDIDATES\n",
      "     3.842     9.986     36311\n",
      "REPEATS\n",
      "     3.028     7.656   1278288\n",
      "UNCOMMON REPEATS\n",
      "     5.927    11.408    303868\n",
      "NON-ERB UNCOMMON REPEATS\n",
      "     6.794    10.382    172779\n",
      "MISLEADING INDUCTION\n",
      "     4.810     9.906    182416\n",
      "CANDIDATE ERB\n",
      "     3.463     8.765     30721\n",
      "NFERB UR\n",
      "     6.862    10.338    158612\n",
      "\n",
      "\n",
      "Running experiment baseline\n",
      "OVERALL\n",
      "     4.623    11.468   3000000\n",
      "CANDIDATES\n",
      "     4.421    11.424    163129\n",
      "LATER CANDIDATES\n",
      "     4.748    11.149     36300\n",
      "REPEATS\n",
      "     3.203     8.591   1278282\n",
      "UNCOMMON REPEATS\n",
      "     6.525    11.767    303828\n",
      "NON-ERB UNCOMMON REPEATS\n",
      "     7.255    10.874    172782\n",
      "MISLEADING INDUCTION\n",
      "     4.956    10.432    182420\n",
      "CANDIDATE ERB\n",
      "     4.494    10.615     30711\n",
      "NFERB UR\n",
      "     7.309    10.839    158614\n",
      "\n",
      "\n",
      "Running experiment ev+\n",
      "OVERALL\n",
      "     4.209    10.506   3000000\n",
      "CANDIDATES\n",
      "     3.279    11.460    163129\n",
      "LATER CANDIDATES\n",
      "     1.199     7.852     36300\n",
      "REPEATS\n",
      "     2.522     5.705   1278282\n",
      "UNCOMMON REPEATS\n",
      "     3.986    11.652    303828\n",
      "NON-ERB UNCOMMON REPEATS\n",
      "     5.597    10.103    172782\n",
      "MISLEADING INDUCTION\n",
      "     4.871    10.142    182420\n",
      "CANDIDATE ERB\n",
      "     0.229     0.622     30711\n",
      "NFERB UR\n",
      "     5.787     9.724    158614\n",
      "\n",
      "\n",
      "Running experiment pth-k+\n",
      "OVERALL\n",
      "     4.271    10.825   3000000\n",
      "CANDIDATES\n",
      "     3.315    11.333    163143\n",
      "LATER CANDIDATES\n",
      "     1.242     7.388     36299\n",
      "REPEATS\n",
      "     2.638     6.602   1278262\n",
      "UNCOMMON REPEATS\n",
      "     4.567    13.291    303812\n",
      "NON-ERB UNCOMMON REPEATS\n",
      "     6.245    10.867    172761\n",
      "MISLEADING INDUCTION\n",
      "     4.859     9.891    182397\n",
      "CANDIDATE ERB\n",
      "     0.329     0.957     30710\n",
      "NFERB UR\n",
      "     6.422    10.420    158595\n",
      "\n",
      "\n",
      "Running experiment eq+\n",
      "OVERALL\n",
      "     4.249    10.425   3000000\n",
      "CANDIDATES\n",
      "     3.336    11.221    163147\n",
      "LATER CANDIDATES\n",
      "     1.391     7.649     36312\n",
      "REPEATS\n",
      "     2.621     5.869   1278308\n",
      "UNCOMMON REPEATS\n",
      "     4.362    11.268    303884\n",
      "NON-ERB UNCOMMON REPEATS\n",
      "     5.740     9.691    172800\n",
      "MISLEADING INDUCTION\n",
      "     4.798     9.696    182439\n",
      "CANDIDATE ERB\n",
      "     0.509     1.616     30722\n",
      "NFERB UR\n",
      "     5.912     9.354    158631\n"
     ]
    }
   ],
   "source": [
    "experiments = make_experiments(make_make_corr(ExactSampler()))\n",
    "for exp in [\"unscrubbed\", \"ev\", \"pth-k\", \"eq\", \"all\", \"baseline\", \"ev+\", \"pth-k+\", \"eq+\"]:\n",
    "    print(f\"\\n\\nRunning experiment {exp}\")\n",
    "    run_experiment(experiments, exp, 10000, \"\", 0, False, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 vs 1.6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Parroting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Subgraph Ablation Attribution, we need to rebuild the experiments using a special sampler so we get several resamples for a single dataset example. Then, we call `main.py:run_experiment` with a few special arguments specifying for the results to be saved in the `results` directory following a specific naming convention. This generates several pickles, which we then pass onto CUI using the `utils.py:compare_saa_in_cui`. In CUI, the \"Comparison(example)\" parameter should be set to either \"facet\" or some specific comparison (in the code below, we have only one). We do this here for the scrubbing of head 1.5 to reproduce the corresponding figure from the writeup.\n",
    "\n",
    "Note: For producing such figures for several experiments and/or indices in a row, the file `get_data.py` provides a simple command-line interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVERALL\n",
      "     3.691    10.733    300000\n",
      "CANDIDATES\n",
      "     3.495    11.479     33000\n",
      "LATER CANDIDATES\n",
      "     1.411     9.299      7000\n",
      "REPEATS\n",
      "     2.065     6.115    139000\n",
      "UNCOMMON REPEATS\n",
      "     2.893    11.646     44000\n",
      "NON-ERB UNCOMMON REPEATS\n",
      "     4.569    14.252     21000\n",
      "MISLEADING INDUCTION\n",
      "     5.006    11.791     21000\n",
      "CANDIDATE ERB\n",
      "     0.171     0.076      6000\n",
      "NFERB UR\n",
      "     4.717    14.504     20000\n",
      "OVERALL\n",
      "     4.099    11.085    300000\n",
      "CANDIDATES\n",
      "     4.060    11.102     33000\n",
      "LATER CANDIDATES\n",
      "     2.199     7.920      7000\n",
      "REPEATS\n",
      "     2.595     7.968    139000\n",
      "UNCOMMON REPEATS\n",
      "     4.726    12.179     44000\n",
      "NON-ERB UNCOMMON REPEATS\n",
      "     6.766     9.850     21000\n",
      "MISLEADING INDUCTION\n",
      "     4.860    12.889     21000\n",
      "CANDIDATE ERB\n",
      "     1.113     0.978      6000\n",
      "NFERB UR\n",
      "     6.850    10.125     20000\n",
      "Composable UI server already running on localhost:6789 in this Python process\n",
      "http://interp-tools.redwoodresearch.org/#/tensors/untitled?port=6789&url=localhost\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h1><a href=\"http://interp-tools.redwoodresearch.org/#/tensors/untitled?port=6789&url=localhost\" target=\"_blank\">Link</a><script>window.open(http://interp-tools.redwoodresearch.org/#/tensors/untitled?port=6789&url=localhost,\"_blank\")</script></h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiments = make_experiments(make_make_corr(FixedSampler(4)))\n",
    "run_experiment(\n",
    "    experiments,\n",
    "    \"unscrubbed\",\n",
    "    1000,\n",
    "    \"unscrubbed_saa_4\",\n",
    "    0,\n",
    "    False,\n",
    "    False,\n",
    ")\n",
    "run_experiment(\n",
    "    experiments,\n",
    "    \"scrub-1.5\",\n",
    "    1000,\n",
    "    \"scrub-1.5_saa_4\",\n",
    "    0,\n",
    "    False,\n",
    "    False,\n",
    ")\n",
    "compare_saa_in_cui([(\"unscrubbed\", \"scrub-1.5\")])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing attention patterns for the unscrubbed model can be done using interp-tools directly, since the model we studied is available there as \"attention_only_two_layers_untied\". However, since we will need to introduce our custom attention visualization function later on, and the test input shown in the writeup is a specific one, we demonstrate usage of our attention visualization functions here.\n",
    "\n",
    "Similarly to the above code block, we need to save the results of our experiments by calling `main.py:run_experiment` with specific arguments. Then, we invoke `utils.py:compare_attns_in_cui` to process the relevant pickles into CUI. `compare_attns_in_cui` differs from `compare_saa_in_cui` in that each \"comparison\" can also be a single experiment (rather than only a pair of experiments), so we can visualize the attentions for that particular experiment without comparing them to another one's. In the future, we will likely generalize `compare_saa_in_cui` in a similar fashion."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = make_experiments(make_make_corr(FixedSampler(4)))\n",
    "run_experiment(\n",
    "    experiments,\n",
    "    \"unscrubbed\",\n",
    "    1000,\n",
    "    \"unscrubbed_saa_4\",\n",
    "    0,\n",
    "    True,\n",
    "    False,\n",
    ")\n",
    "compare_attns_in_cui([\"unscrubbed\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ad467ea6ce724cf68e3e9460a7b5d30a577b9574f7d768503ed98371d079a52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
